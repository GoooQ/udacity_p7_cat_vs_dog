{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 猫狗大战 毕业项目\n",
    "猫狗大战是一个图片分类项目。根据所给出的图片，判断图片中的是猫还是狗。对于这一二分类问题，我们采用基于预训练网络的迁移学习的方法构造模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始\n",
    "导入一切并我们设置所使用的GPU。\n",
    "- dev0: GTX1070Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengjun/.conda/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#import utilities\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm  \n",
    "from time import time\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import *\n",
    "from keras import backend as K\n",
    "from resnext import *\n",
    "from helper import *\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据文件处理\n",
    "训练数据包括12500张猫的图片和12500张狗的图片。我们为数据文件建立symbol link并划分为训练集和验证集，所使用的方法参考了[这里](https://github.com/ypwhs/dogs_vs_cats)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为数据建立symbol link并划分为训练集和验证集\n",
    "def prepare_data_file():    \n",
    "    work_dir  = os.getcwd()\n",
    "    train_dir = work_dir + \"/train/\"\n",
    "    test_dir  = work_dir + \"/test/\"\n",
    "    data_dir  = work_dir + \"/data/\"\n",
    "    \n",
    "    if(os.path.exists(data_dir)):\n",
    "        shutil.rmtree(data_dir)\n",
    "        \n",
    "    split_train_dir = work_dir+\"/data/train\"\n",
    "    split_test_dir  = work_dir+\"/data/test\"\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "    os.mkdir(split_train_dir)\n",
    "    os.mkdir(split_train_dir+\"/dog\")\n",
    "    os.mkdir(split_train_dir+\"/cat\")\n",
    "    os.mkdir(split_test_dir)\n",
    "    os.mkdir(split_test_dir+\"/test\")\n",
    "        \n",
    "    train_files = os.listdir(train_dir)    \n",
    "    num_train_files = len(train_files)\n",
    "    for i in tqdm(range(num_train_files)):\n",
    "        file = train_files[i]\n",
    "        if \"dog\" in file.split('.'):\n",
    "            os.symlink(train_dir+file, split_train_dir+\"/dog/\"+file)\n",
    "        else:\n",
    "            os.symlink(train_dir+file, split_train_dir+\"/cat/\"+file)\n",
    "    \n",
    "    test_files = os.listdir(test_dir)    \n",
    "    num_test_files = len(test_files)\n",
    "    for i in tqdm(range(num_test_files)):\n",
    "        file = test_files[i]\n",
    "        os.symlink(test_dir+file, split_test_dir+\"/test/\"+file)\n",
    "        \n",
    "    return split_train_dir, split_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:00<00:00, 168475.44it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 188212.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#为数据连理symbol-link\n",
    "train_data_dir, test_data_dir = prepare_data_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基准模型\n",
    "我们考虑融合三个模型作为我们的基准模型。这里我们选择五种模型作为候选模型：\n",
    "- [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "- [Xception](https://arxiv.org/abs/1610.02357)\n",
    "- [InceptionV3](https://arxiv.org/abs/1512.00567)\n",
    "- [DenseNet](https://arxiv.org/abs/1608.06993)\n",
    "- [NASNet](https://arxiv.org/abs/1707.07012)\n",
    "\n",
    "对于融合模型的方法和过程，我们参考了[这里](https://github.com/ypwhs/dogs_vs_cats)的内容，在此感谢@培神的分享。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络并训练\n",
    "基于这些导出的特征，我们构建并训练猫狗问题的网络。在此，我们只需要给出全连接层即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件中读取特征向量和标签\n",
    "np.random.seed(2017)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for filename in [\"feature_densenet169.h5\", \"feature_xception.h5\", \"feature_inception_v3.h5\"]:\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        X_train.append(np.array(h['train']))\n",
    "        X_test.append(np.array(h['test']))\n",
    "        Y_train = np.array(h['label'])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#建立顶层网络结构\n",
    "input_tensor = Input(X_train.shape[1:])\n",
    "x = Dropout(0.5)(input_tensor)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input_tensor, x)\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 0.0636 - acc: 0.9808 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01469, saving model to mergenet2-best_weight.h5\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0129 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01469 to 0.01288, saving model to mergenet2-best_weight.h5\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 0.0163 - acc: 0.9949 - val_loss: 0.0113 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01288 to 0.01131, saving model to mergenet2-best_weight.h5\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0111 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01131 to 0.01107, saving model to mergenet2-best_weight.h5\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0094 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01107 to 0.00942, saving model to mergenet2-best_weight.h5\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.0120 - acc: 0.9956 - val_loss: 0.0090 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00942 to 0.00905, saving model to mergenet2-best_weight.h5\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0091 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.0089 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00905 to 0.00890, saving model to mergenet2-best_weight.h5\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0088 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00890 to 0.00877, saving model to mergenet2-best_weight.h5\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.0110 - acc: 0.9961 - val_loss: 0.0089 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0090 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0102 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0098 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0089 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0099 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0097 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1s 43us/step - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0091 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0076 - acc: 0.9972 - val_loss: 0.0100 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0090 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0092 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "#训练模型并保存顶层网络参数\n",
    "filepath=\"mergenet2-best_weight.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2, shuffle=True,\n",
    "         callbacks=callbacks_list)\n",
    "model.save_weights(\"mergenet2-dog-cat.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在测试集上进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 1s 45us/step\n"
     ]
    }
   ],
   "source": [
    "#导入模型权重，并进行预测    \n",
    "model.load_weights('mergenet2-best_weight.h5')\n",
    "y_test = model.predict(X_test, verbose=1)\n",
    "y_test = y_test.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengjun/.conda/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1  0.995\n",
       "1   2  0.995\n",
       "2   3  0.995\n",
       "3   4  0.995\n",
       "4   5  0.005\n",
       "5   6  0.005\n",
       "6   7  0.005\n",
       "7   8  0.005\n",
       "8   9  0.005\n",
       "9  10  0.005"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(test_data_dir, (224, 224), shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_test[i])\n",
    "\n",
    "df.to_csv('pred-mergenet2.csv', index=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.03797"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
